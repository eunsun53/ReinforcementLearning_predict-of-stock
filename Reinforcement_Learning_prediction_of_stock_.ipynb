{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning_prediction of stock .ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/r4+RmPAwgAziX9osWiVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunsun53/ReinforcementLearning_predict-of-stock/blob/main/Reinforcement_Learning_prediction_of_stock_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<Reinforcement Learning을 이용한 주식 가격 예측>\n",
        "> * model- free RL, Q-Learning 적용 \n",
        "> * agent가 (state, action) 에 따른 reward로 피드백을 받아 최대의 보상을 받을 수 있도록 Q 함수를 업데이트 할 수 있도록 policy를 찾아내도록 하는 것 "
      ],
      "metadata": {
        "id": "24B7-dK2kyuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#드라이브-코랩 연동 \n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo-ZnQOXqqVa",
        "outputId": "ca55ed81-8a56-48e1-b79a-4983ee7b05e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-TL2PZCqzYm",
        "outputId": "c5002b60-8e52-430c-c904-15174c07bee4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd DQL_trading"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tey6-sxUk177",
        "outputId": "4317e7b3-3bca-4fb3-956f-8f5403bede24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/dataset/DQL_trading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mTHzHzMnkhKg"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from sklearn.preprocessing import StandardScaler "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "3GbkUzT_rDJp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data \n",
        "msft = pd.read_csv('daily_MSFT.csv', usecols = ['close']) # 주식 종가만 가져옴 \n",
        "ibm = pd.read_csv('daily_IBM.csv', usecols = ['close'])\n",
        "qcom = pd.read_csv('daily_QCOM.csv', usecols=['close'])"
      ],
      "metadata": {
        "id": "iGI7j9Vbk5M6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msft.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SJ11UEUGk7Ma",
        "outputId": "1df221d5-0182-4e8d-c9d3-be727be12968"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   close\n",
              "0  85.52\n",
              "1  85.40\n",
              "2  85.51\n",
              "3  85.50\n",
              "4  85.52"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb0c4851-6688-4a16-80a2-795d50ef5794\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>85.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>85.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>85.52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb0c4851-6688-4a16-80a2-795d50ef5794')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb0c4851-6688-4a16-80a2-795d50ef5794 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb0c4851-6688-4a16-80a2-795d50ef5794');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msft['close'].values[::-1] # 데이터 순서 reverse "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NjVZWeMk87j",
        "outputId": "84b638ad-5cf0-42d8-87e3-acc16d2063f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([116.56, 112.62, 113.81, ...,  85.51,  85.4 ,  85.52])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([msft['close'].values[::-1],\n",
        "                   ibm['close'].values[::-1],\n",
        "                   qcom['close'].values[::-1]])\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFMjXtJdk_La",
        "outputId": "cb4dce15-e6f9-4b74-f123-702f48607684"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[116.56  , 112.62  , 113.81  , ...,  85.51  ,  85.4   ,  85.52  ],\n",
              "       [116.    , 112.06  , 116.    , ..., 152.5   , 152.83  , 153.0385],\n",
              "       [179.3   , 162.1   , 158.    , ...,  64.73  ,  64.3   ,  64.52  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.around(data) # 반올림 \n",
        "data.shape #(3, 4526)\n",
        "\n",
        "train_data = data[:, :3526] #[:][:3526]\n",
        "test_data = data[:, 3526:]"
      ],
      "metadata": {
        "id": "lARW-JB4lBSD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Environment\n",
        "> - state: [(해당 주식 갯수, 해당 주식 1주당 가격), 계좌 잔액]\n",
        "> - action: sell(0), hold(1), buy(2)\n",
        "> - reward: 보상을 주는 policy는 다양하게 상황에 맞게 정의함 \n",
        ">> - 1. action이후의 포트폴리오의 가치가 높으면 +1, 낮으면 -1 feedback \n",
        ">> - 2. action이후의 포트폴리오의 가치가 높으면 +1, 낮으면 -100 feedback\n",
        ">> - 3. action이후의 포트폴리오의 가치의 차이만큼 feedback "
      ],
      "metadata": {
        "id": "mkbKcnJIlFme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1L1cwMFsdph",
        "outputId": "877ce833-80f1-4d94-ed1d-44d3b2d301ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3526)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new = train_data.max(axis = 1)\n",
        "print(new.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5jvd0unzJTx",
        "outputId": "901490ae-db41-40cb-8ce1-5d67bd49a599"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlemm-Obzhep",
        "outputId": "acc2348b-523a-4a29-ea13-e472c37efff5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[117. 216. 179.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(gym.Env):\n",
        "  def __init__(self, train_data, init_invest = 20000):\n",
        "    self.stock_price_history = np.around(train_data)\n",
        "    self.n_stock, self,n_step = self.stock_price_history.shape # 주식 종류 갯수, 각 요소의 갯수 \n",
        "\n",
        "    self.init_invest = init_invest \n",
        "    self.cur_step = None\n",
        "    self.stock_owned = None\n",
        "    self.stock_price = None\n",
        "    self.cash_in_hand = None\n",
        "\n",
        "    #define action space \n",
        "    self.action_space = spaces.Discrete(3**self.n_stock) # 액션의 갯수 정의 \n",
        "\n",
        "    # 데이터 관찰 \n",
        "    stock_max_price = self.stock_price_history.max(axis = 1) #(3, )\n",
        "    stock_range = [[0, init_invest *2 // max] for mx in stock_max_price]\n",
        "    price_range = [[0, mx] for mx in stock_max_price]\n",
        "    cash_in_hand_range = [[0, init_invest * 2]]\n",
        "    \n",
        "    #define state space \n",
        "    self.observation_space = spaces.MultiDiscrete(stock_range+ price_range + cash_in_hand_range)\n",
        "\n",
        "    self._seed()\n",
        "    self._reset()\n",
        "\n",
        "  def _seed(self, seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]\n",
        "\n",
        "\n",
        "  def _reset(self):\n",
        "    self.cur_step = 0\n",
        "    self.stock_owned = [0] * self.n_stock\n",
        "    self.stock_price = self.stock_price_history[:, self.cur_step]\n",
        "    self.cash_in_hand = self.init_invest\n",
        "    return self._get_obs()\n",
        "\n",
        "\n",
        "  def _step(self, action):\n",
        "    assert self.action_space.contains(action)\n",
        "    prev_val = self._get_val()\n",
        "    self.cur_step += 1\n",
        "    self.stock_price = self.stock_price_history[:, self.cur_step] # update price\n",
        "    self._trade(action)\n",
        "    cur_val = self._get_val()\n",
        "    reward = cur_val - prev_val\n",
        "    done = self.cur_step == self.n_step - 1\n",
        "    info = {'cur_val': cur_val}\n",
        "    return self._get_obs(), reward, done, info\n",
        "\n",
        "\n",
        "  def _get_obs(self):\n",
        "    obs = []\n",
        "    obs.extend(self.stock_owned)\n",
        "    obs.extend(list(self.stock_price))\n",
        "    obs.append(self.cash_in_hand)\n",
        "    return obs\n",
        "\n",
        "\n",
        "  def _get_val(self):\n",
        "    return np.sum(self.stock_owned * self.stock_price) + self.cash_in_hand\n",
        "\n",
        "\n",
        "  def _trade(self, action):\n",
        "    # all combo to sell(0), hold(1), or buy(2) stocks\n",
        "    action_combo = map(list, itertools.product([0, 1, 2], repeat=self.n_stock))\n",
        "    action_vec = action_combo[action]\n",
        "\n",
        "    # one pass to get sell/buy index\n",
        "    sell_index = []\n",
        "    buy_index = []\n",
        "    for i, a in enumerate(action_vec):\n",
        "      if a == 0:\n",
        "        sell_index.append(i)\n",
        "      elif a == 2:\n",
        "        buy_index.append(i)\n",
        "\n",
        "    # two passes: sell first, then buy; might be naive in real-world settings\n",
        "    if sell_index:\n",
        "      for i in sell_index:\n",
        "        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
        "        self.stock_owned[i] = 0\n",
        "    if buy_index:\n",
        "      can_buy = True\n",
        "      while can_buy:\n",
        "        for i in buy_index:\n",
        "          if self.cash_in_hand > self.stock_price[i]:\n",
        "            self.stock_owned[i] += 1 # buy one share\n",
        "            self.cash_in_hand -= self.stock_price[i]\n",
        "          else:\n",
        "            can_buy = False\n"
      ],
      "metadata": {
        "id": "_gq91k9MlISv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# modeling \n",
        "def mlp(n_obs, n_action, n_hidden_layer=1, n_neuron_per_layer=32,\n",
        "        activation='relu', loss='mse'):\n",
        "  \"\"\" A multi-layer perceptron \"\"\"\n",
        "  model = Sequential()\n",
        "  model.add(Dense(n_neuron_per_layer, input_dim=n_obs, activation=activation))\n",
        "  for _ in range(n_hidden_layer):\n",
        "    model.add(Dense(n_neuron_per_layer, activation=activation))\n",
        "  model.add(Dense(n_action, activation='linear'))\n",
        "  model.compile(loss=loss, optimizer=Adam())\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "metadata": {
        "id": "jwG2YOweGjgx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "from sklearn.preprocessing import StandardScaler # 평균 _, 분산_ 로 표준화 \n",
        "\n",
        "def get_scaler(env):\n",
        "  \"\"\" Takes a env and returns a scaler for its observation space \"\"\"\n",
        "  low = [0] * (env.n_stock * 2 + 1)\n",
        "\n",
        "  high = []\n",
        "  max_price = env.stock_price_history.max(axis=1)\n",
        "  min_price = env.stock_price_history.min(axis=1)\n",
        "  max_cash = env.init_invest * 3 # 3 is a magic number...\n",
        "  max_stock_owned = max_cash // min_price\n",
        "  for i in max_stock_owned:\n",
        "    high.append(i)\n",
        "  for i in max_price:\n",
        "    high.append(i)\n",
        "  high.append(max_cash)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit([low, high]) # Compute the mean and std to be used for later scaling\n",
        "  return scaler"
      ],
      "metadata": {
        "id": "CmqU9BBOIvfN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque \n",
        "import random \n",
        "\n",
        "# Deep-Q agent \n",
        "class DQNAgent(object):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size \n",
        "    self.action_size = action_size \n",
        "    self.memory = deque(maxlen = 2000)\n",
        "    self.gamma = 0.95 #discount rate \n",
        "    self.epsilon = 1.0 \n",
        "    self.epsilon_min = 0.01 \n",
        "    self.epsilon_decay = 0.995 \n",
        "    self.model = mlp(state_size, action_size)\n",
        "\n",
        "  # 각 행동마다 state, action, reward, next_state,, 등의 정보 저장 \n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  #state에 따른 action 취해줌 \n",
        "  def act(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return random.randrange(self.action_size)\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "  # batch 사이즈에 해당할때마다 agent 학습(reinforcement learning)\n",
        "  def replay(self, batch_size=32):\n",
        "    \"\"\" vectorized implementation; 30x speed up compared with for loop \"\"\"\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    states = np.array([tup[0][0] for tup in minibatch])\n",
        "    actions = np.array([tup[1] for tup in minibatch])\n",
        "    rewards = np.array([tup[2] for tup in minibatch])\n",
        "    next_states = np.array([tup[3][0] for tup in minibatch])\n",
        "    done = np.array([tup[4] for tup in minibatch])\n",
        "\n",
        "    # Q(s', a)\n",
        "    target = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "    # end state target is reward itself (no lookahead)\n",
        "    target[done] = rewards[done]\n",
        "\n",
        "    # Q(s, a)\n",
        "    target_f = self.model.predict(states) # 현재 state에 대해 model에서 예측한 state\n",
        "    # make the agent to approximately map the current state to future discounted reward\n",
        "    target_f[range(batch_size), actions] = target\n",
        "\n",
        "    self.model.fit(states, target_f, epochs=1, verbose=0) # 현재 state와 예측값을 가지고 훈련 진행 \n",
        "\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "\n",
        "\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)\n"
      ],
      "metadata": {
        "id": "VAVoztNUHXbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym \n",
        "from gym import spaces \n",
        "from gym.utils import seeding \n",
        "import itertools\n",
        "import argparse"
      ],
      "metadata": {
        "id": "4IM0tvyGlDcQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-e', '--episode', type=int, default=2000,\n",
        "                    help='number of episode to run')\n",
        "parser.add_argument('-b', '--batch_size', type=int, default=32,\n",
        "                    help='batch size for experience replay')\n",
        "parser.add_argument('-i', '--initial_invest', type=int, default=20000,\n",
        "                    help='initial investment amount')\n",
        "parser.add_argument('-m', '--mode', type=str, required=True,\n",
        "                    help='either \"train\" or \"test\"')\n",
        "parser.add_argument('-w', '--weights', type=str, help='a trained model weights')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "if not os.path.exists('weights'):\n",
        "    os.makedirs('weights')\n",
        "\n",
        "if not os.path.exists('portfolio_val'):\n",
        "    os.makedirs('portfolio_val')"
      ],
      "metadata": {
        "id": "sJeG4KcVO3ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## main ##\n",
        "env = TradingEnv(train_data, args.initial_invest)\n",
        "state_size = env.observation_space.shape \n",
        "action_size = env.action_space.shape \n",
        "\n",
        "agent = DQNAgent(state_size, action_size) #DQL의 agent 생성 \n",
        "\n",
        "scaler = get_scaler(env) # 표준화 객체 생성 \n",
        "\n",
        "portfolio_value = []\n",
        "\n",
        "# test 모드일 경우 test_data 불러오기 및 env 생성 \n",
        "if args.mode == 'test':\n",
        "    # remake the env with test data\n",
        "    env = TradingEnv(test_data, args.initial_invest)\n",
        "    # load trained weights\n",
        "    agent.load(args.weights)\n",
        "    # when test, the timestamp is same as time when weights was trained\n",
        "    timestamp = re.findall(r'\\d{12}', args.weights)[0]"
      ],
      "metadata": {
        "id": "CcKS0i0WrQ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# 에피소드마다 학습 반복 \n",
        "for e in range(args.episode):\n",
        "  state = env.reset()\n",
        "  state = scaler.transform([state])\n",
        "  for time in range(env.n_step):\n",
        "    action = agent.act(state) # state에 따라 취해줄 action\n",
        "    next_state, reward, done, info = env.step(action) # action 실행 \n",
        "    next_state = scaler.transform([next_state]) # next_state 표준화 진행 \n",
        "    if args.mode == 'train':\n",
        "      agent.remember(state, action, reward, next_state, done)\n",
        "    state = next_state # state 갱신 \n",
        "    if done:\n",
        "      print(\"episode: {}/{}, episode end value: {}\".format(\n",
        "        e + 1, args.episode, info['cur_val']))\n",
        "      portfolio_value.append(info['cur_val']) # 에피소드가 끝날 때의 포트폴리오 가치를 기록합니다.\n",
        "      break\n",
        "    if args.mode == 'train' and len(agent.memory) > args.batch_size: #에피소드의 수가 배치사이즈만큼 도달했을 때 agent 강화학습 진행 \n",
        "      agent.replay(args.batch_size)\n",
        "  if args.mode == 'train' and (e + 1) % 10 == 0:  # weights를 중간중간에 저장합니다.\n",
        "    agent.save('weights/{}-dqn.h5'.format(timestamp))\n",
        "\n",
        "  # 포트폴리오 가치 변화를 저장합니다.\n",
        "  with open('portfolio_val/{}-{}.p'.format(timestamp, args.mode), 'wb') as fp:\n",
        "    pickle.dump(portfolio_value, fp)"
      ],
      "metadata": {
        "id": "fy6q4-MPE3rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[참고 깃허브](https://github.com/llSourcell/Q-Learning-for-Trading)"
      ],
      "metadata": {
        "id": "49geFg5krqNG"
      }
    }
  ]
}